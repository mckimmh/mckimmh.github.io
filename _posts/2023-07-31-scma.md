---
layout: post
title: "Statistical Challenges in Modern Astronomy VIII"
date: 2023-07-31 09:45:00 +0100
categories: jekyll update
---
The conference [Statistical Challenges in Modern Astronomy VIII](https://sites.psu.edu/astrostatistics/scma8/) (SCMA) at Penn State University, 12-16 June 2023, was a great experience!

## What is modern astronomy about?

## What *are* the statistical challenges in modern astronomy?

### Lots of Data

Astronomers are preparing for a number of astronomical surveys that are launching in the next few years. One of these is the [Legacy Survey of Space and Time](https://rubinobservatory.org/explore/lsst), conducted by the [Rubin Observatory](https://rubinobservatory.org/about). This survey will produce about 20 terabytes of data every night for ten years! Whilst it's great to have such large quantities of data, it poses computational problems.

1. **You may want to repeat the same analysis on each data-set. Then the more data-sets you have, the greater the cost of doing all the computation.**
James Buchanan presented work on fitting galaxy models using MCMC for inference. He was able to use cluster computing to fit roughly 100,000 models using over 1000 CPU cores. Not all researchers have access to this much computing power, but James Buchanan works at the Lawrence Livermore National Laboratory, which has world-class computing facilities (primarily used for research on nuclear weapons). Wang et al. (2023) also look at the problem of fitting a particular model to each galaxy that will be observed by the Rubin Observatory. They calculate that so many galaxies will be observed, that for their model, fitting it to each galaxy using traditional methods, such as MCMC (Goodman and Weare, 2010) or Nested Sampling (Skilling, 2004), will require 100 billion CPU hours. They propose using Simulation-Based Inference instead, to achieve massive speed-ups. 

1. **In a Bayesian hierarchical model, the number of model parameters scales with the size of the data, making inference increasingly difficult.** For example, Karchev et al (2022) aim to use data on $$10^5$$ supernovae to infer two cosmological parameters. However, the hierarchical model that they use has three latent variables per supernovae, so the model has over $$3 \times 10^5$$ parameters in total. This makes Bayesian inference very difficult. Their proposed solution is based on Simulation-Based Inference.

### Complex Models

1. **Accounting for instrument effects.** Usually, one must not only model the mechanism by which some quantity (such as light) was produced, but also how it was observed. This is certainly the case within my own research, where we must allow for all the ways that *instrument effects* have affected the observed data. Kyle Cranmer, who worked at the Large Hadron Collider and contributed to the discovery of the Higgs boson in 2012, made this point in his talk on Simulation-Based Inference. The detectors for particle physics processes are incredibly complex and can be modeled with stochastic simulations with billions of latent variables (Cranmer et al., 2020).

2. **Complicated, High-Dimensional Data**. Nguyen et al., (2022) provides an interesting example of an astronomical model of complicated, high-dimensional data. Nguyen et al., (2022) aims to learn about dark matter by studying Dwarf galaxies (small, galaxies dominated by dark matter). The data are the kinematics of stars in dwarf galaxies. This poses a statistical challenge, because the data may consist of tens of thousands of galaxies, with galaxies consisting of different numbers of stars (100 on average) and each star being represented by 6 co-ordinates (position and velocity). Nguyen et al., (2022) use Simulation-Based Inference. An interesting component of their method is that they use a Graph Neural Network to automatically extract 128 features to represent each galaxy. Thus each galaxy is compressed to be represented by a summary statistic of fixed dimension.

3. **Transdimensional Models.** Jeffrey Regier presented a good example of a model for which inference is complex. The motivation for the work is that there will be a greater density of light sources in upcoming astronomical surveys. This is because these surveys will look deeper into space, so more light sources will overlap. These "blends" of overlapping light sources create ambiguity in the interpretation of the data. Hansen et al. (2022) present a model (and inference method) for inferring the number of stars and galaxies in an image, their locations, fluxes and shapes. The mathematical formulation of the model is rather beautifully simple (it may be expressed in 5 equations). However the model is complex from the inference point of view, because the inference problem is *transdimensional*. That is, the total number of latent variables depends on the number of sources in the image, which is itself a latent variable. *Variational Inference* is used to to infer the model's parameters. Jeffrey Regier found that in comparison to inference via MCMC, Variational Inference was more accurate and 10,000 times faster.

## What are some solutions?

Of the many technical developments discussed at the conference, I will mention 5 key ideas:
1. Simulation-Based Inference (SBI)
2. Variational Inference (VI)
3. Normalizing Flows (NFs)
4. GPU computing
5. Cluster computing

These 5 key ideas may be subdivided into two categories: the first three (SBI, VI, RL) are algorithms for more efficient inference or learning. The last two (GPU and Cluster computing) refer to the use of more powerful computer hardware. Although I am making this distinction between algorithms and computer hardware, it's worth noting that the two are related. That is, more efficient algorithms are often those that are better able to make use of more powerful computer hardware.

### Simulation-Based Inference

Simulation-Based Inference refers to a class of methods for Bayesian inference when the likelihood is intractable but simulating data from the model is possible. See Lueckmann et al. (2021) for a concise overview of four categories of SBI methods:
1. Monte Carlo ABC
2. Likelihood Estimation
3. Posterior Estimation
4. Ratio Estimation

SBI was prominent at SCMA; I've already mentioned talks by Wang, Karchev, Cranmer and Nguyen on SBI, but there were also talks on the subject by Justin Alsing and Ann Lee. The work presented by Ann Lee (Masserano et al., 2023) was in part motivated by Hermans et al., (2022), which gives empirical evidence that SBI methods can produce overconfident posterior approximations. Masserano et al., (2023) provide a method that may be used to correct overconfident posterior regions.

### Variational Inference

VI (Jordan et al., 1999) approximates the posterior distribution by using optimisation to find a member of a family of probability distributions that is close (in terms of Kullback-Leibler divergence) to the posterior. See Blei et al., 2017 for a review. Early work on VI uses a mean-field variational family, which means that the distribution used to approximate the posterior distribution is a product of mutually independent distributions. This is of limited use, since it is then impossible to learn anything about how the correlation structure of the target posterior distribution. However, more recent research uses as the variational family the rich class of distributions defined by Normalizing flows (Rezende and Mohamed, 2015, Kingma et al., 2016). Using this class of distributions as the variational family allows for far greater flexibility.

### Normalizing Flows

Quoting (Rezende and Mohamed, 2015): "*A normalizing flow describes the transformation of a probability density through a sequence of invertible mappings. By repeatedly applying the rule for change of variables, the initial density ‘flows’ through the sequence of invertible mappings. At the end of this sequence we obtain a valid probability distribution and hence this type of flow is referred to as a normalizing flow*". NFs may be used for both SBI (Papamakarios et al., 2018) and VI (Rezende and Mohamed, 2015, Kingma et al., 2016). In addition, Kaze Wong presented work on using NFs to speed-up MCMC sampling (Wong et al., 2022). NFs uses NN. NFs are trained and evaluated using GPUs.

### GPU Computing

### Cluster Computing

## References

Cranmer, K., Brehmer, J., and Louppe, G. (2020). The frontier of simulation-based inference. Proceedings of the National Academy of Sciences, 117(48):30055–30062.

Goodman, J. and Weare, J. (2010). Ensemble samplers with affine invariance. Communications in applied mathematics and computational science, 5(1):65–80.

Hansen, D., Mendoza, I., Liu, R., Pang, Z., Zhao, Z., Avestruz, C., and Regier, J. (2022). Scalable Bayesian Inference for Detection and Deblending in Astronomical Images.

Hermans, J., Delaunoy, A., Rozet, F., Wehenkel, A., Begy, V., and Louppe, G. (2022). A Trust Crisis In Simulation-Based Inference? Your Posterior Approximations Can Be Unfaithful.

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., and Saul, L. K. (1999). An Introduction to Variational Methods for Graphical Models. Machine learning, 37:183–233.

Karchev, K., Trotta, R., and Weniger, C. (2022). SICRET: Supernova Ia Cosmology with truncated marginal neural Ratio EsTimation. Monthly Notices of the Royal Astronomical Society, 520(1):1056–1072.

Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. (2016). Improved Variational Inference with Inverse Autoregressive Flow. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.

Lueckmann, J.-M., Boelts, J., Greenberg, D. S., Gon ̧calves, P. J., and Macke, J. H. (2021). Benchmarking Simulation-Based Inference.

Masserano, L., Dorigo, T., Izbicki, R., Kuusela, M., and Lee, A. B. (2023). Simulation-Based Inference with Waldo: Confidence Regions by Leveraging Prediction Algorithms or Posterior Estimators for Inverse Problems.

Papamakarios, G., Nalisnick, E., Rezende, D. J., Mohamed, S., and Lakshminarayanan, B. (2021). Normalizing Flows for Probabilistic Modeling and Inference. Journal of Machine Learning Research, 22(57):1–64

Papamakarios, G., Sterratt, D. C., and Murray, I. (2018). Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows

Rezende, D. and Mohamed, S. (2015). Variational Inference with Normalizing Flows. In Bach, F. and Blei, D., editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1530–1538, Lille, France. PMLR.

Skilling, J. (2004). Nested Sampling. AIP Conference Proceedings, 735(1):395–405.

Wang, B., Leja, J., Villar, V. A., and Speagle, J. S. (2023). SBI++: Flexible, Ultra-fast Likelihood-free Inference Customized for Astronomical Applications. The Astrophysical Journal Letters, 952(1):L10

Wong, K. W. K., Gabri ́e, M., and Foreman-Mackey, D. (2022). flowMC: Normalizing-flow enhanced sampling package for probabilistic inference in Jax.
